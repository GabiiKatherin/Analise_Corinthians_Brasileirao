{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs+Pj1O/kJmMEeunegx/MR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabiiKatherin/Analise_Corinthians_Brasileirao/blob/main/Analise_de_dados_Corinthians.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Instalando bibliotecas:"
      ],
      "metadata": {
        "id": "I8qMeaUSWCaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3pvi9XqdGss",
        "outputId": "5bc59046-26c8-4c0c-84a2-e0e866fbdc39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "#instalando bibliotecas:\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import requests\n",
        "#O BeautifulSoup é pra analisar e extrair dados do HTML ou XML\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Webscraping e consolidação dos dados:"
      ],
      "metadata": {
        "id": "n1ejkjMHWNIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#definindo as urls com os dados:\n",
        "url_gols = 'https://www.espn.com.br/futebol/time/estatisticas/_/id/874/liga/BRA.1/vista'\n",
        "url_cartoes = 'https://www.espn.com.br/futebol/time/estatisticas/_/id/874/liga/BRA.1/vista/cartoes'\n",
        "url_rendimento = 'https://www.espn.com.br/futebol/time/estatisticas/_/id/874/liga/BRA.1/vista/rendimento'"
      ],
      "metadata": {
        "id": "2y6ANZmSdagZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#requisicao HTTP\n",
        "response = requests.get(url_gols)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "metadata": {
        "id": "J9yuXs4hdr8O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#procurando a tabela pela tag <table> - me certifiquei disso inspecionando o elemento na página\n",
        "tables = soup.find_all('table')"
      ],
      "metadata": {
        "id": "9b66SwNNeEeQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converte a tabela num Dataframe:\n",
        "#a função read_html é do Pandas e basicamente converte o conteúdo HTML da tala num Dataframe.\n",
        "df_list = []\n",
        "for table in tables:\n",
        "    df = pd.read_html(str(table))[0]\n",
        "    df_list.append(df)"
      ],
      "metadata": {
        "id": "GCcvgR8HePbn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "hSmGWLHEelUu",
        "outputId": "dadb6f89-39c1-453d-b223-3656d682f403"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d4a71b52f896>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa etapa acusou um erro, dizendo que não havia nada na minha tabela, provavelmente porque os dados não foram capturados como eu gostaria. Então, fui testar e fazer uma exibição do que estava sendo capturado e identificar o problema."
      ],
      "metadata": {
        "id": "igE1sXzEfS3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#verificar o que foi capturado:\n",
        "print(soup.prettify()[:1000])  #top 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06-HMELke54Q",
        "outputId": "de719b7b-edee-4499-848e-95afb55dfd0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
            "<html>\n",
            " <head>\n",
            "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
            "  <title>\n",
            "   ERROR: The request could not be satisfied\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <h1>\n",
            "   403 ERROR\n",
            "  </h1>\n",
            "  <h2>\n",
            "   The request could not be satisfied.\n",
            "  </h2>\n",
            "  <hr noshade=\"\" size=\"1px\"/>\n",
            "  Request blocked.\n",
            "We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n",
            "  <br clear=\"all\"/>\n",
            "  If you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n",
            "  <br clear=\"all\"/>\n",
            "  <hr noshade=\"\" size=\"1px\"/>\n",
            "  <pre>\n",
            "Generated by cloudfront (CloudFront)\n",
            "Request ID: BuYZyPR_CSEemxqUkgi30HT8MWosbV-BSxbw76LsQFMnCowZa2AgRQ==\n",
            "</pre>\n",
            "  <address>\n",
            "  </address>\n",
            " </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isso aqui é interessante! O que aconteceu foi que a minha requisição foi bloqueada pelo servidor. Provavelmente, porque ele está evitando acessos automatizados por scripts de scraping, uma medida de segurança super válida, inclusive.\n",
        "\n",
        "Decidi testar mudar o User-Agent e em vez de usar o request padrão, imitar um navegador real."
      ],
      "metadata": {
        "id": "wnwYMlZ_fiM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define um cabeçalho HTTP para simular uma requisição feita por um navegador.\n",
        "#O User-Agent especifica as informações do navegador e do sistema operacional\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "#Faz uma requisição GET na url que eu defini antes\n",
        "response_g = requests.get(url_gols, headers=headers)\n",
        "response_c = requests.get(url_cartoes, headers=headers)\n",
        "response_r = requests.get(url_rendimento, headers=headers)\n",
        "\n",
        "#Converte o HTML num soup - facilita a manipulação e extração dos dados.\n",
        "soup_g = BeautifulSoup(response_g.text, 'html.parser')\n",
        "soup_c = BeautifulSoup(response_c.text, 'html.parser')\n",
        "soup_r = BeautifulSoup(response_r.text, 'html.parser')"
      ],
      "metadata": {
        "id": "X2ZVEzHOgHS2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encontrando minha tabela:\n",
        "tables = soup_g.find_all('table')\n",
        "print(f\"Total de tabelas encontradas: {len(tables)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtvWs_vMgrlh",
        "outputId": "0a2fc5ac-282f-43c5-9f44-9ce17a768f7b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tabelas encontradas: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como eu tenho mais de uma tabela, vou precisar de uma estrutura de repetição para capturar todas elas."
      ],
      "metadata": {
        "id": "e2NAUBxitVVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t, table in enumerate(tables):\n",
        "    df_gols = pd.read_html(str(table))[0]\n",
        "    print(f\"Tabela {t+1}:\")\n",
        "    print(df_gols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz65-Y5JhcvD",
        "outputId": "1d472e81-8528-479a-81a6-475a1b2e0658"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabela 1:\n",
            "     POS              Nome   J  G\n",
            "0    1.0     Rodrigo Garro  25  5\n",
            "1    2.0      Yuri Alberto  20  4\n",
            "2    NaN      Ángel Romero  19  4\n",
            "3    4.0              Cacá  21  3\n",
            "4    5.0    Wesley Ribeiro  23  2\n",
            "5    NaN      Talles Magno   6  2\n",
            "6    7.0     Igor Coronado  22  1\n",
            "7    NaN      Matheus Bidu  12  1\n",
            "8    NaN           Giovane  11  1\n",
            "9    NaN    Pedro Henrique  10  1\n",
            "10  11.0           Raniele  23  0\n",
            "11   NaN              Hugo  22  0\n",
            "12   NaN        Mateuzinho  22  0\n",
            "13   NaN       Breno Bidon  18  0\n",
            "14   NaN      Félix Torres  17  0\n",
            "15   NaN        Pedro Raúl  17  0\n",
            "16   NaN            Fagner  12  0\n",
            "17   NaN        Hugo Souza  11  0\n",
            "18   NaN              Ryan  11  0\n",
            "19   NaN  Gustavo Henrique  10  0\n",
            "20   NaN     André Ramalho   9  0\n",
            "21   NaN   Matheus Donelli   7  0\n",
            "22   NaN     Carlos Miguel   6  0\n",
            "23   NaN           Charles   6  0\n",
            "Tabela 2:\n",
            "     POS              Nome   J  A\n",
            "0    1.0     Rodrigo Garro  25  4\n",
            "1    NaN              Hugo  22  4\n",
            "2    3.0      Yuri Alberto  20  3\n",
            "3    4.0     Igor Coronado  22  2\n",
            "4    NaN              Cacá  21  2\n",
            "5    6.0    Wesley Ribeiro  23  1\n",
            "6    NaN            Fagner  12  1\n",
            "7    NaN      Matheus Bidu  12  1\n",
            "8    NaN    Pedro Henrique  10  1\n",
            "9   10.0           Raniele  23  0\n",
            "10   NaN        Mateuzinho  22  0\n",
            "11   NaN      Ángel Romero  19  0\n",
            "12   NaN       Breno Bidon  18  0\n",
            "13   NaN      Félix Torres  17  0\n",
            "14   NaN        Pedro Raúl  17  0\n",
            "15   NaN        Hugo Souza  11  0\n",
            "16   NaN              Ryan  11  0\n",
            "17   NaN           Giovane  11  0\n",
            "18   NaN  Gustavo Henrique  10  0\n",
            "19   NaN     André Ramalho   9  0\n",
            "20   NaN   Matheus Donelli   7  0\n",
            "21   NaN     Carlos Miguel   6  0\n",
            "22   NaN           Charles   6  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-a67a650f79a4>:2: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  df_gols = pd.read_html(str(table))[0]\n",
            "<ipython-input-34-a67a650f79a4>:2: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  df_gols = pd.read_html(str(table))[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subiu uma alerta de uso no pd.read_html(). E como minhas tabelas são pequenas, eu consegui notar que estão faltando dados. Por isso, vou utilizar o StringIO para tratar as strings como arquivo, mesmo."
      ],
      "metadata": {
        "id": "zkWb-4WKvcdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tables = soup_c.find_all('table')\n",
        "\n",
        "df_cartoes = pd.read_html(str(tables[0]))[0]\n",
        "print(\"Tabela de Cartoes:\")\n",
        "print(df_cartoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4qvRm5euQ39",
        "outputId": "5cdc42ef-ad6e-4602-ddc0-11ad48ff0026"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabela de Cartoes:\n",
            "     POS              Nome   J  CA  CV  pts\n",
            "0    1.0  Gustavo Henrique  10   5   1    8\n",
            "1    2.0           Raniele  23   6   0    6\n",
            "2    NaN              Cacá  21   3   1    6\n",
            "3    4.0      Alex Santana   8   4   0    4\n",
            "4    NaN     Igor Coronado  22   4   0    4\n",
            "5    NaN           Caetano   5   1   1    4\n",
            "6    7.0           Charles   6   3   0    3\n",
            "7    NaN      Félix Torres  17   3   0    3\n",
            "8    NaN     André Ramalho   9   3   0    3\n",
            "9   10.0              Hugo  22   2   0    2\n",
            "10   NaN        Mateuzinho  22   2   0    2\n",
            "11   NaN        Hugo Souza  11   2   0    2\n",
            "12  13.0            Maycon   2   1   0    1\n",
            "13   NaN          Léo Mana   5   1   0    1\n",
            "14   NaN      Matheus Bidu  12   1   0    1\n",
            "15   NaN            Fagner  12   1   0    1\n",
            "16  17.0     Renato Santos   0   0   0    0\n",
            "17   NaN    Diego Palacios   0   0   0    0\n",
            "18   NaN      Felipe Longo   0   0   0    0\n",
            "19   NaN   Matheus Donelli   7   0   0    0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-be9983ee573f>:3: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  df_cartoes = pd.read_html(str(tables[0]))[0]\n"
          ]
        }
      ]
    }
  ]
}